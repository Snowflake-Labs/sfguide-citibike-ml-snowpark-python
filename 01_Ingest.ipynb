{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Ingest Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incremental and Bulk Extract, Load and Transform\n",
    "We expect to get new data every month which we will incrementally load.  Here we will create some functions to wrap the ELT functions from the Data Engineer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.snowpark_connection import snowpark_connect\n",
    "session, state_dict = snowpark_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags import elt as ELT\n",
    "\n",
    "import snowflake.snowpark as snp\n",
    "import uuid \n",
    "\n",
    "ELT.reset_database(session=session, state_dict=state_dict, prestaged=False)\n",
    "\n",
    "state_dict.update({'download_base_url': 'https://s3.amazonaws.com/tripdata/',\n",
    "                       'load_table_name': 'RAW_',\n",
    "                       'trips_table_name': 'TRIPS',\n",
    "                       'load_stage_name': 'LOAD_STAGE'\n",
    "                  })\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will test the ELT functions.  We pick a couple of files representing the various schema and file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_end2 = '202102-citibike-tripdata.csv.zip'\n",
    "file_name_end1 = '201402-citibike-tripdata.zip'\n",
    "file_name_end3 = '202003-citibike-tripdata.csv.zip'\n",
    "\n",
    "files_to_download = [file_name_end1, file_name_end2, file_name_end3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/201402-citibike-tripdata.zip\n",
      "Putting 2014-02 - Citi Bike trip data.csv to stage: LOAD_STAGE/schema1/\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202003-citibike-tripdata.csv.zip\n",
      "Putting 202003-citibike-tripdata.csv to stage: LOAD_STAGE/schema1/\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202102-citibike-tripdata.csv.zip\n",
      "Putting 202102-citibike-tripdata.csv to stage: LOAD_STAGE/schema2/\n",
      "CPU times: user 33.3 s, sys: 626 ms, total: 33.9 s\n",
      "Wall time: 44.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "load_stage_names, files_to_load = ELT.extract_trips_to_stage(session=session, \n",
    "                                                            files_to_download=files_to_download, \n",
    "                                                            download_base_url=state_dict['download_base_url'], \n",
    "                                                            load_stage_name=state_dict['load_stage_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ELT logic requires downloading data to the local system in order to unzip as well as upload the file to a stage.  This can be really slow depending on network speed.  Later we will provide a __bulk-load option that uses data already in gzip format in order to speed up the hands-on-lab__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 218 ms, sys: 14.5 ms, total: 233 ms\n",
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "files_to_load['schema1']=[file+'.gz' for file in files_to_load['schema1']]\n",
    "files_to_load['schema2']=[file+'.gz' for file in files_to_load['schema2']]\n",
    "\n",
    "stage_table_names = ELT.load_trips_to_raw(session=session, \n",
    "                                          files_to_load=files_to_load, \n",
    "                                          load_stage_names=load_stage_names, \n",
    "                                          load_table_name=state_dict['load_table_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.9 ms, sys: 129 Âµs, total: 33 ms\n",
      "Wall time: 7.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trips_table_name = ELT.transform_trips(session=session, \n",
    "                                       stage_table_names=stage_table_names, \n",
    "                                       trips_table_name=state_dict['trips_table_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are two separate schemas we will create two separate ingest paths.  For that we will want to separate the files into two groups like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'schema1': ['202004-citibike-tripdata.csv.gz'],\n",
       " 'schema2': ['202102-citibike-tripdata.csv.gz']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "files_to_ingest=['202004-citibike-tripdata.csv.zip', '202102-citibike-tripdata.csv.zip']\n",
    "schema1_download_files = list()\n",
    "schema2_download_files = list()\n",
    "schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "for file_name in files_to_ingest:\n",
    "    file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "    if file_start_date < schema2_start_date:\n",
    "        schema1_download_files.append(file_name.replace('.zip','.gz'))\n",
    "    else:\n",
    "        schema2_download_files.append(file_name.replace('.zip','.gz'))\n",
    "        \n",
    "files_to_load = {'schema1': schema1_download_files, 'schema2': schema2_download_files}\n",
    "files_to_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create the incremental ELT function as well as a bulk load function.  The bulk ingest function wraps the incremental ingest with a full set of data to bootstrap the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dags/ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dags/ingest.py\n",
    "def incremental_elt(session, \n",
    "                    state_dict:dict, \n",
    "                    files_to_ingest:list, \n",
    "                    download_base_url,\n",
    "                    use_prestaged=False) -> str:\n",
    "    \n",
    "    import dags.elt as ELT\n",
    "    from datetime import datetime\n",
    "\n",
    "    load_stage_name=state_dict['load_stage_name']\n",
    "    load_table_name=state_dict['load_table_name']\n",
    "    trips_table_name=state_dict['trips_table_name']\n",
    "    \n",
    "    if use_prestaged:\n",
    "        print(\"Skipping extract.  Using provided bucket for pre-staged files.\")\n",
    "        \n",
    "        schema1_download_files = list()\n",
    "        schema2_download_files = list()\n",
    "        schema2_start_date = datetime.strptime('202102', \"%Y%m\")\n",
    "\n",
    "        for file_name in files_to_ingest:\n",
    "            file_start_date = datetime.strptime(file_name.split(\"-\")[0], \"%Y%m\")\n",
    "            if file_start_date < schema2_start_date:\n",
    "                schema1_download_files.append(file_name.replace('.zip','.gz'))\n",
    "            else:\n",
    "                schema2_download_files.append(file_name.replace('.zip','.gz'))\n",
    "        \n",
    "        \n",
    "        load_stage_names = {'schema1':load_stage_name+'/schema1/', 'schema2':load_stage_name+'/schema2/'}\n",
    "        files_to_load = {'schema1': schema1_download_files, 'schema2': schema2_download_files}\n",
    "    else:\n",
    "        print(\"Extracting files from public location.\")\n",
    "        load_stage_names, files_to_load = ELT.extract_trips_to_stage(session=session, \n",
    "                                                                    files_to_download=files_to_ingest, \n",
    "                                                                    download_base_url=download_base_url, \n",
    "                                                                    load_stage_name=load_stage_name)\n",
    "        \n",
    "        files_to_load['schema1']=[file+'.gz' for file in files_to_load['schema1']]\n",
    "        files_to_load['schema2']=[file+'.gz' for file in files_to_load['schema2']]\n",
    "\n",
    "\n",
    "    print(\"Loading files to raw.\")\n",
    "    stage_table_names = ELT.load_trips_to_raw(session=session, \n",
    "                                              files_to_load=files_to_load, \n",
    "                                              load_stage_names=load_stage_names, \n",
    "                                              load_table_name=load_table_name)    \n",
    "    \n",
    "    print(\"Transforming records to trips table.\")\n",
    "    trips_table_name = ELT.transform_trips(session=session, \n",
    "                                           stage_table_names=stage_table_names, \n",
    "                                           trips_table_name=trips_table_name)\n",
    "    return trips_table_name\n",
    "\n",
    "def bulk_elt(session, \n",
    "             state_dict:dict,\n",
    "             download_base_url, \n",
    "             use_prestaged=False) -> str:\n",
    "    \n",
    "    #import dags.elt as ELT\n",
    "    from dags.ingest import incremental_elt\n",
    "    \n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    #Create a list of filenames to download based on date range\n",
    "    #For files like 201306-citibike-tripdata.zip\n",
    "    date_range1 = pd.period_range(start=datetime.strptime(\"201306\", \"%Y%m\"), \n",
    "                                 end=datetime.strptime(\"201612\", \"%Y%m\"), \n",
    "                                 freq='M').strftime(\"%Y%m\")\n",
    "    file_name_end1 = '-citibike-tripdata.zip'\n",
    "    files_to_extract = [date+file_name_end1 for date in date_range1.to_list()]\n",
    "\n",
    "    #For files like 201701-citibike-tripdata.csv.zip\n",
    "    date_range2 = pd.period_range(start=datetime.strptime(\"201701\", \"%Y%m\"), \n",
    "                                 end=datetime.strptime(\"201912\", \"%Y%m\"), \n",
    "                                 freq='M').strftime(\"%Y%m\")\n",
    "    \n",
    "    file_name_end2 = '-citibike-tripdata.csv.zip'\n",
    "    \n",
    "    files_to_extract = files_to_extract + [date+file_name_end2 for date in date_range2.to_list()]        \n",
    "\n",
    "    trips_table_name = incremental_elt(session=session, \n",
    "                                       state_dict=state_dict, \n",
    "                                       files_to_ingest=files_to_extract, \n",
    "                                       use_prestaged=use_prestaged,\n",
    "                                       download_base_url=download_base_url)\n",
    "    \n",
    "    return trips_table_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incremental ELT function allows us to specify one or more files to extract, load and transform.  Lets try it with a couple of examples.  Start with a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files from public location.\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202001-citibike-tripdata.csv.zip\n",
      "Putting 202001-citibike-tripdata.csv to stage: LOAD_STAGE/schema1/\n",
      "Loading files to raw.\n",
      "Transforming records to trips table.\n",
      "CPU times: user 20.8 s, sys: 392 ms, total: 21.2 s\n",
      "Wall time: 54.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt\n",
    "from dags.elt import reset_database\n",
    "from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "reset_database(session=session, state_dict=state_dict, prestaged=False)\n",
    "\n",
    "incremental_elt(session=session, \n",
    "                state_dict=state_dict, \n",
    "                files_to_ingest=['202001-citibike-tripdata.csv.zip'], \n",
    "                download_base_url=state_dict['download_base_url'],\n",
    "                use_prestaged=False)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may need to ingest a list of multiple files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files from public location.\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202002-citibike-tripdata.csv.zip\n",
      "Putting 202002-citibike-tripdata.csv to stage: LOAD_STAGE/schema1/\n",
      "Downloading and unzipping: https://s3.amazonaws.com/tripdata/202102-citibike-tripdata.csv.zip\n",
      "Putting 202102-citibike-tripdata.csv to stage: LOAD_STAGE/schema2/\n",
      "Loading files to raw.\n",
      "Transforming records to trips table.\n",
      "CPU times: user 26.3 s, sys: 587 ms, total: 26.9 s\n",
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt\n",
    "from dags.elt import reset_database\n",
    "from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "reset_database(session=session, state_dict=state_dict, prestaged=False)\n",
    "\n",
    "incremental_elt(session=session, \n",
    "                state_dict=state_dict, \n",
    "                files_to_ingest=['202002-citibike-tripdata.csv.zip', '202102-citibike-tripdata.csv.zip'], \n",
    "                download_base_url=state_dict['download_base_url'],\n",
    "                use_prestaged=False)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These load functions will default to loading from the public citibike data set.  However, we may want to be able to specify files already pre-downloaded into a different S3 bucket.  The functions assume the files are in gzip format in that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extract.  Using provided bucket for pre-staged files.\n",
      "Loading files to raw.\n",
      "Transforming records to trips table.\n",
      "CPU times: user 299 ms, sys: 9.31 ms, total: 308 ms\n",
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from dags.ingest import incremental_elt\n",
    "from dags.elt import reset_database\n",
    "from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "session, state_dict = snowpark_connect('./include/state.json')\n",
    "\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "reset_database(session=session, state_dict=state_dict, prestaged=True)\n",
    "\n",
    "incremental_elt(session=session, \n",
    "                state_dict=state_dict, \n",
    "                files_to_ingest=['202001-citibike-tripdata.csv.zip', '202102-citibike-tripdata.csv.zip'],\n",
    "                download_base_url=state_dict['connection_parameters']['download_base_url'],\n",
    "                use_prestaged=True)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also bulk load the entire historical dataset using the following.  This takes at least 30min depending on network speed to your local system. See below for an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from dags.ingest import bulk_elt\n",
    "# from dags.elt import reset_database\n",
    "# from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "# session, state_dict = snowpark_connect('./include/state.json')\n",
    "\n",
    "# session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "# reset_database(session=session, state_dict=state_dict, prestaged=False)\n",
    "\n",
    "# bulk_elt(session=session, \n",
    "#          state_dict=state_dict, \n",
    "#          use_prestaged=False, \n",
    "#          download_base_url='https://s3.amazonaws.com/tripdata/')\n",
    "# session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hands-on-lab we will bulk load from a different S3 bucket where the files are already in gzip format (see below).  \n",
    "\n",
    "For this project we are going back in time and pretending it is January 2020 (so that we can experience the effect of data drift during COVID lockdown).  So this bulk load ingests from an existing bucket with data from June 2013 to January 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping extract.  Using provided bucket for pre-staged files.\n",
      "Loading files to raw.\n",
      "Transforming records to trips table.\n",
      "CPU times: user 288 ms, sys: 4.36 ms, total: 292 ms\n",
      "Wall time: 37.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TRIPS'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from dags.ingest import bulk_elt\n",
    "from dags.elt import reset_database\n",
    "from dags.snowpark_connection import snowpark_connect\n",
    "\n",
    "session, state_dict = snowpark_connect()\n",
    "\n",
    "state_dict.update({'load_table_name': 'RAW_',\n",
    "                   'trips_table_name': 'TRIPS',\n",
    "                   'load_stage_name': 'LOAD_STAGE'\n",
    "                  })\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)\n",
    "\n",
    "reset_database(session=session, state_dict=state_dict, prestaged=True)\n",
    "\n",
    "session.use_warehouse(state_dict['compute_parameters']['fe_warehouse'])\n",
    "\n",
    "bulk_elt(session=session, \n",
    "         state_dict=state_dict, \n",
    "         download_base_url=state_dict['connection_parameters']['download_base_url'],\n",
    "         use_prestaged=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91890993"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.table(state_dict['trips_table_name']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without the need to download locally we ingested ~90 million records in about 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "cforbe"
   }
  ],
  "kernelspec": {
   "display_name": "snowpark_070:Python",
   "language": "python",
   "name": "conda-env-snowpark_070-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "msauthor": "trbye"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
